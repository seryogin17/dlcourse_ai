{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)\n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,3)\n",
    "    return np.sum(x, axis = 1), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[1, 0, 0], [0, 5, 1]], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.00676044, 1.40760596])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([[-5, 0, 5], [1, 2, 3]]))\n",
    "linear_classifer.cross_entropy_loss(probs, [1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.00676044e+00, 9.07957375e-05, 3.80790048e-04])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([[-5, 0, 5], [10, 0, 0], [10, 2, 0]]))\n",
    "linear_classifer.cross_entropy_loss(probs, [1, 0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([[1, 0, 0, 0], [0, 5, 0, 1], [1, 0, 1, 1]], np.float), [0, 1, 1])\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, [0, 1, 1]),\n",
    "                         np.array([[1, 0, 0, 0], [0, 5, 0, 1], [1, 0, 1, 1]], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# after passing the check gradient DO NOT FORGET to find the mean(loss) \n",
    "# by running softmax_with_cross_entropy again\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 4\n",
    "num_classes = 7\n",
    "num_features = 4\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2.301890\n",
      "Epoch 1, loss: 2.300977\n",
      "Epoch 2, loss: 2.300884\n",
      "Epoch 3, loss: 2.300326\n",
      "Epoch 4, loss: 2.301342\n",
      "Epoch 5, loss: 2.301751\n",
      "Epoch 6, loss: 2.300519\n",
      "Epoch 7, loss: 2.300508\n",
      "Epoch 8, loss: 2.300946\n",
      "Epoch 9, loss: 2.300389\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15b35ea7630>]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.131\n",
      "Accuracy after training for 100 epochs:  0.115\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметров.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy achieved: 0.234000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = np.linspace(1e-2, 1e-6, 10)\n",
    "reg_strengths = np.linspace(1e-3, 1e-7, 10)\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0\n",
    "\n",
    "for lr, reg in list(product(learning_rates, reg_strengths)):\n",
    "    classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "    classifier.fit(train_X, train_y, epochs=num_epochs, learning_rate=lr, batch_size=batch_size, reg = reg)\n",
    "    pred = classifier.predict(val_X)\n",
    "    accuracy = multiclass_accuracy(pred, val_y)\n",
    "    if accuracy > best_val_accuracy:\n",
    "        best_classifier = classifier\n",
    "        best_val_accuracy = accuracy\n",
    "        best_params = {'reg': reg, 'lr': lr}\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.198000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
